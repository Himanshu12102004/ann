# Multilayer Perceptron Model (Function Approximator)

This repository contains a custom implementation of a Multilayer Perceptron (MLP) in C++, designed as a function approximator. The MLP model is built from scratch, featuring backpropagation, various activation functions, and gradient descent algorithms.

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Getting Started](#getting-started)
- [Usage](#usage)
- [Examples](#examples)
- [Performance](#performance)
- [Contributing](#contributing)
- [License](#license)

## Introduction

The Multilayer Perceptron (MLP) is a fundamental neural network model used for a variety of tasks, including classification, regression, and function approximation. This project implements a fully functional MLP from scratch, showcasing my understanding of neural networks and C++ programming.

## Features

- **Customizable Architecture**: Define the number of layers and neurons per layer.
- **Backpropagation**: Efficient gradient descent-based learning algorithm.
- **Activation Functions**: Supports various activation functions including sigmoid, tanh, and ReLU.
- **Function Approximation**: Accurate approximation of complex non-linear functions.
- **Learning Rate Adjustment**: Fine-tune the learning rate for optimal performance.
- **Regularization**: Includes options for regularization techniques to prevent overfitting.

## Getting Started

### Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/Himanshu12102004/ann.git
   cd ann
   ```
2. Strating the project on mac: